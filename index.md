---
layout: home
---

<p align="center">
<img src="/files/homepage-banner.png" alt="A digital representation of a human brain split in two halves, with the left side depicting neural pathways and glowing synapses, and the right side illustrating electronic circuits, lights, and digital nodes. The two halves symbolize the convergence of brain and technology." style="height: 250px;">
</p>

Humans are incredible learners---able to rapidly assimilate new information on
the fly to robustly learn new concepts and skills from limited experiences.
In contrast, current Machine Learning (ML) systems can exceed the scale at which
humans learn but are much less data efficient and are difficult and costly to
update in the face of new data. This research program aims to identify key
characteristics of human learning that are not yet realized within artificial
systems, and then explore how to design systems with these capabilities. 

Research in this area has the potential to produce systems that:
- learn more efficiently, yielding a smaller energy and carbon footprint;
- learn from less data, translating into less need for proprietary data and data workers;
- update on the fly, able to adapt users in realtime in response to their inputs;
- learn continually, staying abreast of a constant stream of data without
  forgetting previous capabilities; and
- many others.

## Interested? Get Involved!

Consider participating in one of the two upcoming events sponsored by AAAI:

| **[AAAI-24 Symposium on Human-Like Learning](/aaai24-ss/)** | **[AAAI-24 Tutorial on Probabilistic Concept Formation](/aaai24-tutorial/)** |
|:-------------:|:--------------:|
| ![A futuristic classroom setting illuminated with soft blue lighting showcases a mix of technology and traditional learning. In the center of the room, a large holographic projection of a human head, half-transparent with digital components and circuits, stands prominently, symbolizing the blend of humanity and technology. Around the hologram, two individuals, a man and a woman, stand behind podiums, engaged in a discussion or debate. Students sit at sleek desks equipped with advanced touch-screen monitors, actively interacting with the digital content. Floating holographic books and a digital brain hover in the space, suggesting a deep integration of knowledge and technology. The room's boundaries are framed by bookshelves on one side and a window with a serene view of a green landscape on the other. The overall ambiance combines the traditional elements of a learning environment with state-of-the-art technological advancements.](/files/symposium.png) | ![A detailed illustration depicts a modern classroom or workshop setting with a large digital display at the front. The screen showcases various machine learning concepts, including Bayesian Models, Decision Trees, Nearest Neighbor, and Clustering. Each concept is visually represented by corresponding graphs and models. A tree-like network structure with vibrant nodes radiates from the center of the screen. In front of the display, an instructor, holding a pointer, explains the concepts to an older individual. Below them, a group of diverse individuals sits around a horseshoe-shaped table, equipped with advanced touch-screen monitors, attentively listening and interacting with their devices. The ambiance is studious, with a futuristic touch symbolizing the advanced nature of the subject matter.](/files/cobweb-tutorial.png) |

<!---
As an example of this idea, Large Language Models (LLMs) are a revolutionary
technology, but require substantial data and compute to train, and are
difficult to update post deployment. The primary ways to adjust LLMs are
_prompting_ and _fine tuning_.  While prompting is useful for customizing
behavior, it does not provide a way to more permanently update or change the
behavior of an LLM on repeated use. In contrast, fine tuning does, but runs the
risk of producing (catastrophic) forgetting; where the model gets worse at
previously learned things when training on something new. This is especially
true for cases such as continual learning, where a model might be repeatedly
fine tuned on a stream of new data.  Interestingly humans can learn efficiently
and incrementally from a continual stream of data without experiencing
catastrophic forgetting. Thus, research on human-like learning might explore
how we can create language modeling approaches that support dynamic and
efficient updating on the fly without forgetting. Such a capability would have
a positive real-world impact on how people can interact with and use Artificial
Intelligence (AI) technologies. 
--->

